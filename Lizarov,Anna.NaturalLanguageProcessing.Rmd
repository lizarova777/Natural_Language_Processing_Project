---
title: "Natural Language Processing (NLP)"
author: "Anna Lizarov"
date: "March 16, 2019"
output: html_document
---

## Libraries
```{r}
#Make sure you install and load the following libraries

library(tm)
library(SnowballC)
library(wordcloud)
library(ggplot2)
library(dplyr)
library(tidyr)
library(topicmodels)
```

## Import all document files and the list of weeks file
```{r}
#Create a list of all the files
file.list <- list.files(path="~/LA Process and Theory (HUDK 4051)/Natural Language Processing (NLP)/natural-language-processing/class-notes/", pattern=".csv", full.names = TRUE)

#Loop over file list importing them and binding them together
D1 <- do.call("rbind", lapply(grep(".csv", file.list, value = TRUE), read.csv, header = TRUE, stringsAsFactors = FALSE))

D2 <- read.csv("~/LA Process and Theory (HUDK 4051)/Natural Language Processing (NLP)/natural-language-processing/week-list.csv", header = TRUE)
```

## Step 1 - Clean the htlm tags from your text
```{r}
D1$Notes2 <- gsub("<.*?>", "", D1$Notes)
D1$Notes2 <- gsub("nbsp", "" , D1$Notes2)
D1$Notes2 <- gsub("nbspnbspnbsp", "" , D1$Notes2)
```

## Step 2 - Process text using the tm package
```{r}
#Convert the data frame to the corpus format that the tm package uses
corpus <-VCorpus(VectorSource(D1$Notes2))
#Remove spaces
corpus <- tm_map(corpus, stripWhitespace)
#Convert to lower case
corpus <- tm_map(corpus, tolower)
#Remove pre-defined stop words ('the', 'a', etc)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
#Convert words to stems ("education" = "edu") for analysis, for more info see  http://tartarus.org/~martin/PorterStemmer/
corpus <- tm_map(corpus, stemDocument)
#Remove numbers
corpus <- tm_map(corpus, removeNumbers)
#remove punctuation
corpus <- tm_map(corpus, removePunctuation)
#Convert to plain text for mapping by wordcloud package
corpus <- tm_map(corpus, PlainTextDocument, lazy = TRUE)

#Convert corpus to a term document matrix - so each word can be analyzed individuallly
tdm.corpus <- TermDocumentMatrix(corpus)

#Note: we won't remove plural words here, plural words in English tend to be highly irregular and difficult to extract reliably
```

## Alternative processing - Code has been altered to account for changes in the tm package
```{r}
#Convert the data frame to the corpus format that the tm package uses
corpus <- Corpus(VectorSource(D1$Notes2))
#Remove spaces
corpus <- tm_map(corpus, stripWhitespace)
#Convert to lower case
corpus <- tm_map(corpus, content_transformer(tolower)) 
#Remove pre-defined stop words ('the', 'a', etc)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
#Convert words to stems ("education" = "edu") for analysis, for more info see  http://tartarus.org/~martin/PorterStemmer/
corpus <- tm_map(corpus, stemDocument, lazy=TRUE)
#Remove numbers
corpus <- tm_map(corpus, removeNumbers, lazy=TRUE)
#remove punctuation
corpus <- tm_map(corpus, removePunctuation, lazy=TRUE)
```

What processing steps have you conducted here? Why is this important? Are there any other steps you should take to process your text before analyzing?
```{r}
# Answer: A dataframe which contains texts, in particular, notes written in natural language, was converted into a corpus format to enable computers to process the text and permit parsing. The processing steps were the following: spaces, punctuation, and numbers, have been removed, all the letters have been converted to lowercase, which removes unstructured data. Also, meaningless words (clutter) have been removed, and words have been collapsed into stems since there can be many variations of the same word. This allows comparison of the word profiles across documents.      
```


## Step 3 - Find common words
```{r}
#The tm package can do some simple analysis, like find the most common words
findFreqTerms(tdm.corpus, lowfreq=50, highfreq=Inf)
#We can also create a vector of the word frequencies
word.count <- sort(rowSums(as.matrix(tdm.corpus)), decreasing=TRUE)
word.count <- data.frame(word.count)
```

## Generate a Word Cloud

### ColorBrewer
ColorBrewer is a useful tool to help you choose colors for visualizations that was originally built for cartographers. On the ColorBrewer website (http://colorbrewer2.org/#) you can test different color schemes or see what their preset color schemes look like. This is very useful, especially if you are making images for colorblind individuals. 
```{r}
#Define the colors the cloud will use
col=brewer.pal(6,"Dark2")
#Generate cloud
wordcloud(corpus, min.freq=80, scale=c(5,2),rot.per = 0.25,
          random.color=T, max.word=45, random.order=F,colors=col)
```

## Merge with week list so you have a variable representing weeks for each entry 
```{r}
D3 <- left_join(D1,D2, by="Title")
```

### Create a Term Document Matrix
```{r}
#Convert the data frame to the corpus format that the tm package uses
corpus <-VCorpus(VectorSource(D3$Notes2))
#Remove spaces
corpus <- tm_map(corpus, stripWhitespace)
#Convert to lower case
corpus <- tm_map(corpus, tolower)
#Remove pre-defined stop words ('the', 'a', etc)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
#Convert words to stems ("education" = "edu") for analysis, for more info see  http://tartarus.org/~martin/PorterStemmer/
corpus <- tm_map(corpus, stemDocument)
#Remove numbers
corpus <- tm_map(corpus, removeNumbers)
#remove punctuation
corpus <- tm_map(corpus, removePunctuation)
#Convert to plain text for mapping by wordcloud package
corpus <- tm_map(corpus, PlainTextDocument, lazy = TRUE)
#Convert corpus to a term document matrix - so each word can be analyzed individuallly
tdm.corpus <- TermDocumentMatrix(corpus)
```

# Sentiment Analysis

### Match words in corpus to lexicons of positive & negative words
```{r}
#Upload positive and negative word lexicons
positive <- readLines("positive-words.txt")
negative <- readLines("negative-words.txt")

#Search for matches between each word and the two lexicons
D3$positive <- tm_term_score(tdm.corpus, positive)
D3$negative <- tm_term_score(tdm.corpus, negative)

#Generate an overall pos-neg score for each line
D3$score <- D3$positive - D3$negative

```

## Generate a visualization of the sum of the sentiment score over weeks
```{r}
# Transforming the data frame
D4 <- D3 %>% group_by(week) %>% summarize(sentiment_score=sum(score))
D4 <- na.omit(D4)
# Visualization
g <- ggplot(D4, aes(week, sentiment_score, fill=sentiment_score)) + geom_col() + labs(title = "Change of the Sentiment Score Over Time", x= "Week", y= "Total Sentiment Score") + scale_fill_gradient2(low="green", mid="red", high="blue")
g
```


# LDA Topic Modelling

Using the same csv file you have generated the LDA analysis will treat each row of the data frame as a document. Does this make sense for generating topics?

```{r}
#Answer: It makes sense for generating topics since LDA Topic Modelling assumes that each document is about a certain topic. Thus, this concept can also be applied to students' notes, assuming each note is about a certain topic, which is represented by the word profiles of these notes.  
```


```{r}
#Term Frequency Inverse Document Frequency
dtm.tfi <- DocumentTermMatrix(corpus, control = list(weighting = weightTf))

#Remove very uncommon terms (term freq inverse document freq < 0.1)
dtm.tfi <- dtm.tfi[,dtm.tfi$v >= 0.1]

#Remove non-zero entries
rowTotals <- apply(dtm.tfi , 1, sum) #Find the sum of words in each Document
dtm.tfi   <- dtm.tfi[rowTotals> 0, ] #Divide by sum across rows

lda.model = LDA(dtm.tfi, k = 3, seed = 150)

#Which terms are most common in each topic
terms(lda.model)

#Which documents belong to which topic
topics(lda.model)
```

What does an LDA topic represent? 
```{r}
# Answer: An LDA topic represents the probability of each word appearing in a document of a certain topic. In other words, it is a probability distribution of words in a document. 
```


# Main Visualization

Generate a *single* visualization showing: 

- Sentiment for each week and 
- One important topic for that week

```{r}
D5 <- data.frame(topics(lda.model))
names(D5) = "topic"
D6 <- select(D3,week)
D5$ID = row.names(D5)
D6$ID = row.names(D6)
D7 <- full_join(D6,D5, by = "ID" ) %>% select(-ID)
D7 = na.omit(D7)
ImportantTopic <- function(t) {
   uniqtopic <- unique(t)
   uniqtopic[which.max(tabulate(match(t, uniqtopic)))]
}
D7 <- D7 %>% group_by(week) %>% summarize(MainTopic= ImportantTopic(topic))
D7$MainTopic =  as.character(D7$MainTopic)
D8 <- full_join(D7, D4, by = "week")
```

```{r}
#Visualization
g1 <- ggplot(D8, aes(week, sentiment_score, color=MainTopic)) + geom_point() + scale_x_continuous(breaks = 2:14) + guides(colour=guide_legend(title="Important Topic")) + labs(title = "An Important Topic and Sentiment Per Week", x = "Week", y = "Sentiment")
g1
```
```{r}
# Or
g2<- ggplot(D8, aes(week, sentiment_score, label=MainTopic)) + geom_col(fill="green") +geom_text() + labs(title = "An Important Topic and Sentiment Per Week", x = "Week", y = "Sentiment")
g2

```


